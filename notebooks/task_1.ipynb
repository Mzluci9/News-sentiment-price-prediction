{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efd93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734295c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf98fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e7973d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline Length Statistics:\n",
      " count    1.407328e+06\n",
      "mean     7.312051e+01\n",
      "std      4.073531e+01\n",
      "min      3.000000e+00\n",
      "25%      4.700000e+01\n",
      "50%      6.400000e+01\n",
      "75%      8.700000e+01\n",
      "max      5.120000e+02\n",
      "Name: headline_length, dtype: float64\n",
      "Number of Articles per Publisher (Top 5):\n",
      " publisher\n",
      "Paul Quintaro        228373\n",
      "Lisa Levin           186979\n",
      "Benzinga Newsdesk    150484\n",
      "Charles Gross         96732\n",
      "Monica Gerson         82380\n",
      "Name: count, dtype: int64\n",
      "Publication Date Counts (Top 5):\n",
      " publication_date\n",
      "2011-04-27    1\n",
      "2011-04-28    2\n",
      "2011-04-29    2\n",
      "2011-04-30    1\n",
      "2011-05-01    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the dataset and return a DataFrame.\"\"\"\n",
    "    file_path = \"C:\\\\Users\\\\HP\\\\10 Acadamy PRojects\\\\New folder (1)\\\\News-sentiment-price-prediction\\\\data\\\\raw_analyst_ratings.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def descriptive_statistics(df, output_dir=\"plots/\"):\n",
    "    \"\"\"Perform descriptive statistics on the dataset.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Headline length statistics\n",
    "    df['headline_length'] = df['headline'].apply(len)\n",
    "    print(\"Headline Length Statistics:\\n\", df['headline_length'].describe())\n",
    "    \n",
    "    # Plot headline length distribution\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(df['headline_length'], bins=50, kde=True, color='blue')\n",
    "    plt.title('Distribution of Headline Lengths')\n",
    "    plt.xlabel('Headline Length (Characters)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(os.path.join(output_dir, 'headline_length_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Articles per publisher\n",
    "    publisher_counts = df['publisher'].value_counts()\n",
    "    print(\"Number of Articles per Publisher (Top 5):\\n\", publisher_counts.head())\n",
    "    \n",
    "    # Plot top publishers\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=publisher_counts.head().values, y=publisher_counts.head().index, color='green')\n",
    "    plt.title('Top 5 Publishers by Article Count')\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.ylabel('Publisher')\n",
    "    plt.savefig(os.path.join(output_dir, 'top_publishers.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Publication date trends\n",
    "    df['publication_date'] = pd.to_datetime(df['date'], errors='coerce').dt.date\n",
    "    date_counts = df['publication_date'].value_counts().sort_index()\n",
    "    print(\"Publication Date Counts (Top 5):\\n\", date_counts.head())\n",
    "    \n",
    "    # Plot publication frequency\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    date_counts.plot(kind='line', color='purple')\n",
    "    plt.title('Article Publication Frequency Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.savefig(os.path.join(output_dir, 'publication_frequency.png'))\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    descriptive_statistics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99716e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: new, deal, lower, higher, announces, oil, companies, trading, says, shares\n",
      "Topic 2: trading, sales, higher, movers, yesterday, new, biggest, estimate, update, shares\n",
      "Topic 3: wednesday, hit, thursday, etfs, watch, scheduled, benzinga, moving, session, earnings\n",
      "Topic 4: option, alert, adj, revenue, sees, sales, reports, eps, est, vs\n",
      "Topic 5: initiates, announces, raises, maintains, price, target, upgrades, downgrades, buy, pt\n",
      "Headlines with 'FDA/approval': 15183\n",
      "Headlines with 'price target': 47634\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the dataset and return a DataFrame.\"\"\"\n",
    "    file_path = \"C:\\\\Users\\\\HP\\\\10 Acadamy PRojects\\\\New folder (1)\\\\News-sentiment-price-prediction\\\\data\\\\raw_analyst_ratings.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "def text_analysis(df, output_dir=\"plots/\"):\n",
    "    \"\"\"Perform text analysis and topic modeling on headlines.\"\"\"\n",
    "    if df is None:\n",
    "        print(\"Error: DataFrame is None. Cannot perform text analysis.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Handle missing or invalid headlines\n",
    "    df['headline'] = df['headline'].fillna('').astype(str)\n",
    "\n",
    "    # Preprocess headlines\n",
    "    stop_words = set(stopwords.words('english') + ['stock', 'stocks', 'company', 'market'])\n",
    "    def clean_headline(text):\n",
    "        try:\n",
    "            tokens = word_tokenize(text)\n",
    "            cleaned = [word.lower() for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "            return ' '.join(cleaned)\n",
    "        except Exception as e:\n",
    "            print(f\"Error tokenizing headline: {text}, Error: {e}\")\n",
    "            return ''\n",
    "\n",
    "    df['clean_headline'] = df['headline'].apply(clean_headline)\n",
    "\n",
    "    # Word cloud\n",
    "    all_text = ' '.join(df['clean_headline'])\n",
    "    if all_text.strip():\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(all_text)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of News Headlines')\n",
    "        plt.savefig(os.path.join(output_dir, 'wordcloud.png'))\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Warning: No valid text for word cloud generation.\")\n",
    "\n",
    "    # Topic modeling with LDA\n",
    "    try:\n",
    "        vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "        X = vectorizer.fit_transform(df['clean_headline'])\n",
    "        if X.shape[0] > 0:\n",
    "            lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "            lda.fit(X)\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            for topic_idx, topic in enumerate(lda.components_):\n",
    "                top_words = [feature_names[i] for i in topic.argsort()[-10:]]\n",
    "                print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "        else:\n",
    "            print(\"Warning: No valid documents for LDA after vectorization.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LDA topic modeling: {e}\")\n",
    "\n",
    "    fda_count = df['headline'].str.contains('FDA|fda|approval', case=False, na=False).sum()\n",
    "    price_target_count = df['headline'].str.contains('price target', case=False, na=False).sum()\n",
    "    print(f\"Headlines with 'FDA/approval': {fda_count}\")\n",
    "    print(f\"Headlines with 'price target': {price_target_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    text_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48a372b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: credit, biggest, yesterday, movers, stanley, option, alert, dividend, reports, announces\n",
      "Topic 2: ahead, yesterday, low, high, ceo, highest, set, industry, new, says\n",
      "Topic 3: target, price, market, raises, maintains, update, trading, buy, pt, shares\n",
      "Topic 4: downgrades, upgrades, market, thursday, friday, watch, etfs, moving, session, benzinga\n",
      "Topic 5: revenue, scheduled, sees, estimate, sales, reports, earnings, est, eps, vs\n",
      "Headlines with 'FDA/approval': 15183\n",
      "Headlines with 'price target': 47634\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def text_analysis(df, output_dir=\"plots/\"):\n",
    "    \"\"\"Perform text analysis and topic modeling on headlines.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Preprocess headlines\n",
    "    stop_words = set(stopwords.words('english') + ['stock', 'stocks'])\n",
    "    df['clean_headline'] = df['headline'].apply(\n",
    "        lambda x: ' '.join([word.lower() for word in word_tokenize(x) if word.lower() not in stop_words and word.isalpha()])\n",
    "    )\n",
    "\n",
    "    # Word cloud\n",
    "    all_text = ' '.join(df['clean_headline'])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(all_text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of News Headlines')\n",
    "    plt.savefig(os.path.join(output_dir, 'wordcloud.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Topic modeling with LDA\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "    X = vectorizer.fit_transform(df['clean_headline'])\n",
    "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    lda.fit(X)\n",
    "    \n",
    "    # Print top words per topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[-10:]]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "    # Search for specific topics\n",
    "    fda_count = df['headline'].str.contains('FDA|fda|approval', case=False, na=False).sum()\n",
    "    price_target_count = df['headline'].str.contains('price target', case=False, na=False).sum()\n",
    "    print(f\"Headlines with 'FDA/approval': {fda_count}\")\n",
    "    print(f\"Headlines with 'price target': {price_target_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    text_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a22ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication Spikes (High Activity Days):\n",
      " publication_date\n",
      "2020-02-27    275\n",
      "2020-02-28    381\n",
      "2020-03-06    281\n",
      "2020-03-11    282\n",
      "2020-03-12    973\n",
      "2020-03-19    629\n",
      "2020-03-23    375\n",
      "2020-03-24    160\n",
      "2020-04-09    164\n",
      "2020-04-13    184\n",
      "2020-04-15    186\n",
      "2020-04-16    199\n",
      "2020-04-17    194\n",
      "2020-04-20    180\n",
      "2020-04-21    233\n",
      "2020-04-22    245\n",
      "2020-04-23    265\n",
      "2020-04-24    246\n",
      "2020-04-27    283\n",
      "2020-04-28    317\n",
      "2020-04-29    449\n",
      "2020-04-30    488\n",
      "2020-05-01    385\n",
      "2020-05-04    347\n",
      "2020-05-05    478\n",
      "2020-05-06    531\n",
      "2020-05-07    749\n",
      "2020-05-08    508\n",
      "2020-05-11    485\n",
      "2020-05-12    387\n",
      "2020-05-13    549\n",
      "2020-05-14    536\n",
      "2020-05-15    322\n",
      "2020-05-18    547\n",
      "2020-05-19    332\n",
      "2020-05-20    461\n",
      "2020-05-21    333\n",
      "2020-05-22    286\n",
      "2020-05-26    628\n",
      "2020-05-27    492\n",
      "2020-05-28    465\n",
      "2020-05-29    309\n",
      "2020-06-01    484\n",
      "2020-06-02    361\n",
      "2020-06-03    720\n",
      "2020-06-04    538\n",
      "2020-06-05    932\n",
      "2020-06-08    765\n",
      "2020-06-09    804\n",
      "2020-06-10    806\n",
      "2020-06-11    544\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def time_series_analysis(df, output_dir=\"plots/\"):\n",
    "    \"\"\"Analyze publication frequency and intraday patterns.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Convert date to datetime\n",
    "    df['datetime'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['publication_date'] = df['datetime'].dt.date\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "    # Daily publication frequency\n",
    "    daily_counts = df['publication_date'].value_counts().sort_index()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    daily_counts.plot(kind='line', color='purple')\n",
    "    plt.title('Daily Publication Frequency')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.savefig(os.path.join(output_dir, 'daily_publication_frequency.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Intraday publication frequency\n",
    "    hourly_counts = df['hour'].value_counts().sort_index()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=hourly_counts.index, y=hourly_counts.values, color='orange')\n",
    "    plt.title('Intraday Publication Frequency by Hour (UTC-4)')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.savefig(os.path.join(output_dir, 'intraday_frequency.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Identify spikes\n",
    "    mean_count = daily_counts.mean()\n",
    "    std_count = daily_counts.std()\n",
    "    spikes = daily_counts[daily_counts > mean_count + 2 * std_count]\n",
    "    print(\"Publication Spikes (High Activity Days):\\n\", spikes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    time_series_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a256c157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Publishers:\n",
      " publisher\n",
      "Paul Quintaro        228373\n",
      "Lisa Levin           186979\n",
      "Benzinga Newsdesk    150484\n",
      "Charles Gross         96732\n",
      "Monica Gerson         82380\n",
      "Name: count, dtype: int64\n",
      "Unique Domains:\n",
      " domain\n",
      "benzinga.com      7937\n",
      "gmail.com          139\n",
      "andyswan.com         5\n",
      "investdiva.com       2\n",
      "tothetick.com        2\n",
      "Name: count, dtype: int64\n",
      "News Type Distribution:\n",
      " news_type\n",
      "Other           1261605\n",
      "Earnings          87562\n",
      "Price Target      47457\n",
      "FDA               10704\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def publisher_analysis(df, output_dir=\"plots/\"):\n",
    "    \"\"\"Analyze publishers and news types.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Top publishers\n",
    "    publisher_counts = df['publisher'].value_counts()\n",
    "    print(\"Top 5 Publishers:\\n\", publisher_counts.head())\n",
    "    \n",
    "    # Plot top publishers\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=publisher_counts.head().values, y=publisher_counts.head().index, color='green')\n",
    "    plt.title('Top 5 Publishers by Article Count')\n",
    "    plt.xlabel('Number of Articles')\n",
    "    plt.ylabel('Publisher')\n",
    "    plt.savefig(os.path.join(output_dir, 'top_publishers.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Check for email addresses and extract domains\n",
    "    df['is_email'] = df['publisher'].str.contains(r'@', na=False)\n",
    "    if df['is_email'].any():\n",
    "        df['domain'] = df[df['is_email']]['publisher'].str.split('@').str[1]\n",
    "        domain_counts = df['domain'].value_counts()\n",
    "        print(\"Unique Domains:\\n\", domain_counts.head())\n",
    "    else:\n",
    "        print(\"No email addresses found in publisher names.\")\n",
    "\n",
    "    # Analyze news types\n",
    "    df['news_type'] = df['headline'].apply(\n",
    "        lambda x: 'Earnings' if 'earning' in x.lower() else\n",
    "                  'Price Target' if 'price target' in x.lower() else\n",
    "                  'FDA' if 'fda' in x.lower() else 'Other'\n",
    "    )\n",
    "    news_type_counts = df['news_type'].value_counts()\n",
    "    print(\"News Type Distribution:\\n\", news_type_counts)\n",
    "\n",
    "    # Plot news types by publisher\n",
    "    top_publishers = publisher_counts.head().index\n",
    "    news_type_by_publisher = df[df['publisher'].isin(top_publishers)].groupby(['publisher', 'news_type']).size().unstack(fill_value=0)\n",
    "    news_type_by_publisher.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "    plt.title('News Types by Top Publishers')\n",
    "    plt.xlabel('Publisher')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.savefig(os.path.join(output_dir, 'news_types_by_publisher.png'))\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_data()\n",
    "    publisher_analysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa8cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
